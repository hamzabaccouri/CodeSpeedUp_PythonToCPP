{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6b6d72bb8b10460b9998052c58b1dc24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26e7554c2caa4917827ea7462a7bc90c",
              "IPY_MODEL_03fc0268d29440e09eb85c45d8a5fe67",
              "IPY_MODEL_483bd5f05e0d44379f2f6c820ac12bb5"
            ],
            "layout": "IPY_MODEL_995d95aabc5a47e7b289115264514405"
          }
        },
        "26e7554c2caa4917827ea7462a7bc90c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a415b42e5a584596b49cdfca16fd5545",
            "placeholder": "​",
            "style": "IPY_MODEL_cf7deaaa1380444294b630a81108ca05",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "03fc0268d29440e09eb85c45d8a5fe67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1937928de5245ab9dc625e3d1a0901b",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6791787b6204d538e6a88b9a75334a9",
            "value": 4
          }
        },
        "483bd5f05e0d44379f2f6c820ac12bb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0a17b5cbccb462aafeee02f0903622e",
            "placeholder": "​",
            "style": "IPY_MODEL_99458fdf35bb4ffba2dff83c4c7acb67",
            "value": " 4/4 [01:15&lt;00:00, 17.91s/it]"
          }
        },
        "995d95aabc5a47e7b289115264514405": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a415b42e5a584596b49cdfca16fd5545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf7deaaa1380444294b630a81108ca05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1937928de5245ab9dc625e3d1a0901b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6791787b6204d538e6a88b9a75334a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0a17b5cbccb462aafeee02f0903622e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99458fdf35bb4ffba2dff83c4c7acb67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q9vI6uyrgIRF"
      },
      "outputs": [],
      "source": [
        "!pip install -q python-dotenv requests gradio IPython huggingface_hub transformers bitsandbytes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import json\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import Markdown, display, update_display\n",
        "import gradio as gr\n",
        "import subprocess\n",
        "from huggingface_hub import login, InferenceClient\n",
        "from transformers import AutoTokenizer\n",
        "from google.colab import userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import gc\n",
        "import traceback"
      ],
      "metadata": {
        "id": "OlHszZJugkwj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to HuggingFace Hub\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "OKDfktGVgp1U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code_qwen = \"Qwen/CodeQwen1.5-7B-Chat\""
      ],
      "metadata": {
        "id": "edfWwEW0gzqW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = (\n",
        "    \"You are an assistant that strictly outputs high-performance C++ code from Python. \"\n",
        "    \"Do not include any explanations, markdown formatting, or text other than the C++ code itself. \"\n",
        "    \"Respond only with functional C++ code. \"\n",
        "    \"Ensure the code works on x86 systems and includes all necessary headers like <iostream>, <iomanip>, and <chrono>. \"\n",
        "    \"The C++ response must match the Python implementation's output in both precision and functionality. \"\n",
        "    \"Pay attention to type conversions, floating-point precision, and overflow handling. \"\n",
        ")\n"
      ],
      "metadata": {
        "id": "iMF9TwZiDVYg"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def user_prompt_for(python):\n",
        "    user_prompt = \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. \"\n",
        "    user_prompt += \"Respond only with C++ code; do not explain your work other than a few comments. \"\n",
        "    user_prompt += \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\\n\\n\"\n",
        "    user_prompt += \"Ensure the C++ code produces identical output, avoids any int overflows, and uses appropriate types \"\n",
        "    user_prompt += \"like double for floating-point operations.\"\n",
        "    user_prompt += python\n",
        "    return user_prompt"
      ],
      "metadata": {
        "id": "UbaZbSwrg-QN"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def messages_for(python):\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
        "    ]"
      ],
      "metadata": {
        "id": "kDc9G0ZPjlTH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi = \"\"\"\n",
        "import time\n",
        "\n",
        "def calculate(iterations, param1, param2):\n",
        "    result = 1.0\n",
        "    for i in range(1, iterations+1):\n",
        "        j = i * param1 - param2\n",
        "        result -= (1/j)\n",
        "        j = i * param1 + param2\n",
        "        result += (1/j)\n",
        "    return result\n",
        "\n",
        "start_time = time.time()\n",
        "result = calculate(100_000_000, 4, 1) * 4\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Result: {result:.12f}\")\n",
        "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Tp5RDtZ6hAGh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python_hard = \"\"\"\n",
        "def lcg(seed, a=1664525, c=1013904223, m=2**32):\n",
        "    value = seed\n",
        "    while True:\n",
        "        value = (a * value + c) % m\n",
        "        yield value\n",
        "\n",
        "def max_subarray_sum(n, seed, min_val, max_val):\n",
        "    lcg_gen = lcg(seed)\n",
        "    random_numbers = [next(lcg_gen) % (max_val - min_val + 1) + min_val for _ in range(n)]\n",
        "    max_sum = float('-inf')\n",
        "    for i in range(n):\n",
        "        current_sum = 0\n",
        "        for j in range(i, n):\n",
        "            current_sum += random_numbers[j]\n",
        "            if current_sum > max_sum:\n",
        "                max_sum = current_sum\n",
        "    return max_sum\n",
        "\n",
        "def total_max_subarray_sum(n, initial_seed, min_val, max_val):\n",
        "    total_sum = 0\n",
        "    lcg_gen = lcg(initial_seed)\n",
        "    for _ in range(20):\n",
        "        seed = next(lcg_gen)\n",
        "        total_sum += max_subarray_sum(n, seed, min_val, max_val)\n",
        "    return total_sum\n",
        "\n",
        "# Parameters\n",
        "n = 10000         # Number of random numbers\n",
        "initial_seed = 42 # Initial seed for the LCG\n",
        "min_val = -10     # Minimum value of random numbers\n",
        "max_val = 10      # Maximum value of random numbers\n",
        "\n",
        "# Timing the function\n",
        "import time\n",
        "start_time = time.time()\n",
        "result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"Total Maximum Subarray Sum (20 runs):\", result)\n",
        "print(\"Execution Time: {:.6f} seconds\".format(end_time - start_time))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZZfIBKGGhB1L"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "u75Z13DShERt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_output(cpp):\n",
        "    \"\"\"\n",
        "    Cleans and writes the generated C++ code to a file.\n",
        "\n",
        "    Args:\n",
        "        cpp (str): The raw output from the model, which may contain explanations or markdown.\n",
        "\n",
        "    \"\"\"\n",
        "    # Remove markdown markers and explanations\n",
        "    if \"```cpp\" in cpp:\n",
        "        cpp = cpp.split(\"```cpp\")[-1]  # Extract content after the first ```cpp\n",
        "    if \"```\" in cpp:\n",
        "        cpp = cpp.split(\"```\")[0]  # Stop at the closing ```\n",
        "\n",
        "    # Remove trailing explanatory lines or text\n",
        "    lines = cpp.splitlines()\n",
        "    clean_lines = []\n",
        "    for line in lines:\n",
        "        # Only include lines that look like valid C++ code or comments\n",
        "        if line.strip() and not line.startswith(\"This C++ code\") and not line.startswith(\"Note that\"):\n",
        "            clean_lines.append(line)\n",
        "\n",
        "    # Join cleaned lines and write to file\n",
        "    clean_code = \"\\n\".join(clean_lines)\n",
        "    with open(\"optimized.cpp\", \"w\") as f:\n",
        "        f.write(clean_code.strip())\n"
      ],
      "metadata": {
        "id": "5Z3ByLla44xe"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_cpp(code):\n",
        "    \"\"\"\n",
        "    Compiles and executes C++ code on a PC with x86 architecture.\n",
        "\n",
        "    Args:\n",
        "        code (str): The C++ code to compile and execute.\n",
        "\n",
        "    Returns:\n",
        "        str: Output from the compiled C++ program or an error message.\n",
        "    \"\"\"\n",
        "    write_output(code)  # Save the C++ code to a file\n",
        "    try:\n",
        "        # Adjust compilation flags for x86 architecture\n",
        "        compile_cmd = [\"clang++\", \"-O3\", \"-std=c++17\", \"-march=native\", \"-o\", \"optimized\", \"optimized.cpp\"]\n",
        "        compile_result = subprocess.run(compile_cmd, check=True, text=True, capture_output=True)\n",
        "\n",
        "        # Execute the compiled binary\n",
        "        run_cmd = [\"./optimized\"]\n",
        "        run_result = subprocess.run(run_cmd, check=True, text=True, capture_output=True)\n",
        "\n",
        "        # Return the output of the program\n",
        "        return run_result.stdout\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Return the error message if compilation or execution fails\n",
        "        return f\"An error occurred:\\n{e.stderr}\"\n"
      ],
      "metadata": {
        "id": "VUvZMibl3JGL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def post_process_cpp(cpp_code):\n",
        "    \"\"\"\n",
        "    Cleans the raw C++ code by removing unwanted explanations, markdown formatting, and extraneous text.\n",
        "\n",
        "    Args:\n",
        "        cpp_code (str): Raw C++ code generated by the model.\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned and usable C++ code.\n",
        "    \"\"\"\n",
        "    # Remove markdown markers and explanations\n",
        "    if \"```cpp\" in cpp_code:\n",
        "        cpp_code = cpp_code.split(\"```cpp\")[-1]  # Extract content after the first ```cpp\n",
        "    if \"```\" in cpp_code:\n",
        "        cpp_code = cpp_code.split(\"```\")[0]  # Stop at the closing ```\n",
        "\n",
        "    # Remove lines that don't look like valid C++ code\n",
        "    valid_cpp_lines = []\n",
        "    for line in cpp_code.splitlines():\n",
        "        if not line.strip().startswith(\"//\") and not line.lower().startswith(\"note\") and line.strip():\n",
        "            valid_cpp_lines.append(line)\n",
        "\n",
        "    return \"\\n\".join(valid_cpp_lines).strip()\n"
      ],
      "metadata": {
        "id": "BNc2k9_lAXzc"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_code_qwen(python):\n",
        "    \"\"\"\n",
        "    Converts Python code to high-performance C++ and cleans the output.\n",
        "    Writes the cleaned C++ code to a file and returns the cleaned code and logs.\n",
        "\n",
        "    Args:\n",
        "        python (str): The Python code to convert.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (Cleaned C++ code, Logs for the operation)\n",
        "    \"\"\"\n",
        "    import traceback\n",
        "    import gc\n",
        "    import torch\n",
        "\n",
        "    logs = \"\"\n",
        "    result = \"\"\n",
        "\n",
        "    try:\n",
        "        # Logs to track progress\n",
        "        logs += \"Clearing CUDA cache and garbage collecting...\\n\"\n",
        "\n",
        "        # Clear CUDA cache and garbage collection\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        logs += \"CUDA cache cleared and garbage collection completed.\\n\"\n",
        "\n",
        "        # Load model and tokenizer\n",
        "        model_name = code_qwen  # Replace with the correct Hugging Face model name\n",
        "        logs += f\"Loading model: {model_name}...\\n\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=quant_config,  # Apply quantization configuration here\n",
        "            device_map=\"auto\"  # Automatically map model to available GPUs/CPUs\n",
        "        )\n",
        "        logs += \"Model and tokenizer loaded successfully.\\n\"\n",
        "\n",
        "        # Prepare input text\n",
        "        messages = messages_for(python)\n",
        "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        logs += f\"Prepared input text: {text[:100]}...\\n\"  # Log the first 100 characters\n",
        "\n",
        "        # Tokenize input\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        logs += f\"Using device: {device}\\n\"\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "        logs += \"Input tokenized and sent to the device.\\n\"\n",
        "\n",
        "        # Generate text output\n",
        "        logs += \"Starting code generation...\\n\"\n",
        "        outputs = model.generate(inputs.input_ids, max_new_tokens=3000, do_sample=True)\n",
        "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        logs += \"Code generation completed successfully.\\n\"\n",
        "\n",
        "        # Post-process the output to clean the C++ code\n",
        "        logs += \"Post-processing the generated C++ code...\\n\"\n",
        "        cleaned_result = post_process_cpp(result)\n",
        "        logs += \"Post-processing completed successfully.\\n\"\n",
        "\n",
        "        # Write the cleaned C++ code to a file\n",
        "        logs += \"Writing the cleaned C++ code to 'optimized.cpp'...\\n\"\n",
        "        with open(\"optimized.cpp\", \"w\") as f:\n",
        "            f.write(cleaned_result)\n",
        "        logs += \"Cleaned C++ code written to 'optimized.cpp'.\\n\"\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log error details\n",
        "        logs += f\"An error occurred:\\n{traceback.format_exc()}\"\n",
        "\n",
        "    finally:\n",
        "        # Clean up resources\n",
        "        logs += \"Cleaning up resources...\\n\"\n",
        "        del model\n",
        "        del inputs\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        logs += \"Resources cleaned up successfully.\\n\"\n",
        "\n",
        "    return cleaned_result, logs\n"
      ],
      "metadata": {
        "id": "iroOCGGLErw9"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_code_qwen(python):\n",
        "    import traceback\n",
        "    import gc\n",
        "    import torch\n",
        "\n",
        "    logs = \"\"\n",
        "    result = \"\"\n",
        "\n",
        "    try:\n",
        "        # Logs to track progress\n",
        "        logs += \"Clearing CUDA cache and garbage collecting...\\n\"\n",
        "\n",
        "        # Clear CUDA cache and garbage collection\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        logs += \"CUDA cache cleared and garbage collection completed.\\n\"\n",
        "\n",
        "        # Load model and tokenizer\n",
        "        model_name = code_qwen  # Replace with the correct Hugging Face model name\n",
        "        logs += f\"Loading model: {model_name}...\\n\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=quant_config,  # Apply quantization configuration here\n",
        "            device_map=\"auto\"  # Automatically map model to available GPUs/CPUs\n",
        "        )\n",
        "        logs += \"Model and tokenizer loaded successfully.\\n\"\n",
        "\n",
        "        # Prepare input text\n",
        "        messages = messages_for(python)\n",
        "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        logs += f\"Prepared input text: {text[:100]}...\\n\"  # Log the first 100 characters\n",
        "\n",
        "        # Tokenize input\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        logs += f\"Using device: {device}\\n\"\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "        logs += \"Input tokenized and sent to the device.\\n\"\n",
        "\n",
        "        # Generate text output\n",
        "        logs += \"Starting code generation...\\n\"\n",
        "        outputs = model.generate(inputs.input_ids, max_new_tokens=3000, do_sample=True)\n",
        "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        logs += \"Code generation completed successfully.\\n\"\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log error details\n",
        "        logs += f\"An error occurred:\\n{traceback.format_exc()}\"\n",
        "\n",
        "    finally:\n",
        "        # Clean up resources\n",
        "        logs += \"Cleaning up resources...\\n\"\n",
        "        del model\n",
        "        del inputs\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        logs += \"Resources cleaned up successfully.\\n\"\n",
        "\n",
        "    return result, logs\n"
      ],
      "metadata": {
        "id": "gXSC9_cp0Rgl"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def optimize(python, model):\n",
        "  if model == \"CodeQwen\":\n",
        "    result = stream_code_qwen(python)\n",
        "  else:\n",
        "    raise ValueError(\"Unknown model\")\n",
        "  for stream_so_far in result:\n",
        "    yield stream_so_far"
      ],
      "metadata": {
        "id": "GgZyu8xihbBa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "css = \"\"\"\n",
        ".python {background-color: #306998;}\n",
        ".cpp {background-color: #050;}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5dR0gBrhjFKE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(css=css) as ui:\n",
        "    gr.Markdown(\"## Convert code from Python to C++ using CodeQwen\")\n",
        "    with gr.Row():\n",
        "        python = gr.Textbox(label=\"Python code:\", value=pi, lines=10)  # Input for Python code\n",
        "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)  # Output for converted C++ code\n",
        "    with gr.Row():\n",
        "        convert = gr.Button(\"Convert code\")  # Button to trigger code conversion\n",
        "    with gr.Row():\n",
        "        python_run = gr.Button(\"Run Python\")  # Button to execute the Python code\n",
        "        cpp_run = gr.Button(\"Run C++\")  # Button to execute the C++ code\n",
        "    with gr.Row():\n",
        "        python_out = gr.TextArea(label=\"Python result:\", elem_classes=[\"python\"])  # Display Python execution result\n",
        "        cpp_out = gr.TextArea(label=\"C++ result:\", elem_classes=[\"cpp\"])  # Display C++ execution result\n",
        "    with gr.Row():\n",
        "        logs = gr.TextArea(label=\"Logs\", lines=15)  # TextArea to display logs and errors\n",
        "\n",
        "    # Link the 'convert' button to stream_code_qwen\n",
        "    convert.click(stream_code_qwen, inputs=[python], outputs=[cpp, logs])\n",
        "\n",
        "    # Link the Python execution button\n",
        "    python_run.click(execute_python, inputs=[python], outputs=[python_out])\n",
        "\n",
        "    # Link the C++ execution button\n",
        "    cpp_run.click(execute_cpp, inputs=[cpp], outputs=[cpp_out])\n",
        "\n",
        "ui.launch(inbrowser=True, debug=True, share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553,
          "referenced_widgets": [
            "6b6d72bb8b10460b9998052c58b1dc24",
            "26e7554c2caa4917827ea7462a7bc90c",
            "03fc0268d29440e09eb85c45d8a5fe67",
            "483bd5f05e0d44379f2f6c820ac12bb5",
            "995d95aabc5a47e7b289115264514405",
            "a415b42e5a584596b49cdfca16fd5545",
            "cf7deaaa1380444294b630a81108ca05",
            "b1937928de5245ab9dc625e3d1a0901b",
            "e6791787b6204d538e6a88b9a75334a9",
            "d0a17b5cbccb462aafeee02f0903622e",
            "99458fdf35bb4ffba2dff83c4c7acb67"
          ]
        },
        "id": "OzR99Tgkjs2S",
        "outputId": "95b48785-2a87-4f0f-8bbe-1dffb261c687"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2d95ff37f801b0cbd9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b6d72bb8b10460b9998052c58b1dc24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F4A5TmjT7BSH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}